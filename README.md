# Expressive Audio Augmentation with Multi-Instrument Neural Architectures

**A deep learning framework for sound-conditioned musical creativity**

## Overview

This project explores deep learning approaches for expressive audio augmentation, shifting from pure text-to-music generation toward sound-conditioned creative transformation.
Instead of generating music from text alone, the system listens to audio, extracts expressive characteristics, and produces musically coherent variations grounded in real sonic input.

The system integrates three key technical directions:
- Multi-Instrument Neural-DSP Pipeline (audio-conditioned RNN + Transformer)
- Audio Retrieval-Augmented Generation (RAG) (for extension, coherence, and style grounding)

ðŸ“š Table of Contents

Architecture Summary
Metrics Used
Datasets
Training Pipeline
Project Directory Structure
Scripts & Descriptions
